{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results/visualizations', exist_ok=True)\n",
    "os.makedirs('results/tables', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed = pd.read_csv('results/data/all_results_detailed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# median iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary():\n",
    "    summary_data = []\n",
    "    for (system, dataset), group in df_detailed.groupby(['System', 'Dataset']):\n",
    "        row = {'System': system, 'Dataset': dataset}\n",
    "        \n",
    "        # Calculate medians and IQRs instead of means and std devs\n",
    "        for prefix in ['LR', 'DL']:\n",
    "            for metric in ['MAPE', 'MAE', 'RMSE', 'R2']:\n",
    "                col = f\"{prefix}_{metric}\"\n",
    "                if col in df_detailed.columns:\n",
    "                    # Calculate median\n",
    "                    row[f\"{col}_median\"] = group[col].median()\n",
    "                    \n",
    "                    # Calculate IQR (75th percentile - 25th percentile)\n",
    "                    q1 = group[col].quantile(0.25)\n",
    "                    q3 = group[col].quantile(0.75)\n",
    "                    row[f\"{col}_iqr\"] = q3 - q1\n",
    "        \n",
    "        # Calculate improvements based on medians\n",
    "        for metric in ['MAPE', 'MAE', 'RMSE']:\n",
    "            lr_median = row[f\"LR_{metric}_median\"]\n",
    "            dl_median = row[f\"DL_{metric}_median\"]\n",
    "            # For error metrics, lower is better\n",
    "            row[f\"{metric}_improvement\"] = ((lr_median - dl_median) / lr_median) * 100\n",
    "        \n",
    "        # For R2, higher is better\n",
    "        if row['LR_R2_median'] > 0:\n",
    "            row['R2_improvement'] = ((row['DL_R2_median'] - row['LR_R2_median']) / abs(row['LR_R2_median'])) * 100\n",
    "        else:\n",
    "            row['R2_improvement'] = row['DL_R2_median'] - row['LR_R2_median']\n",
    "        \n",
    "        # Calculate statistical significance\n",
    "        for metric in ['MAPE', 'MAE', 'RMSE', 'R2']:\n",
    "            lr_col = f\"LR_{metric}\"\n",
    "            dl_col = f\"DL_{metric}\"\n",
    "            if lr_col in df_detailed.columns and dl_col in df_detailed.columns:\n",
    "                try:\n",
    "                    if metric == 'R2':\n",
    "                        # For R2, higher is better\n",
    "                        stat, p_value = stats.wilcoxon(group[dl_col], group[lr_col], alternative='greater')\n",
    "                    else:\n",
    "                        # For error metrics, lower is better\n",
    "                        stat, p_value = stats.wilcoxon(group[lr_col], group[dl_col], alternative='greater')\n",
    "                    \n",
    "                    row[f\"{metric}_p_value\"] = p_value\n",
    "                    row[f\"{metric}_significant\"] = p_value < 0.05\n",
    "                except:\n",
    "                    row[f\"{metric}_p_value\"] = np.nan\n",
    "                    row[f\"{metric}_significant\"] = False\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric_plots(df_summary):\n",
    "    metrics = ['MAPE', 'MAE', 'RMSE', 'R2']\n",
    "    \n",
    "    # For each metric, show bar graph with IQR for all systems\n",
    "    for metric in metrics:\n",
    "        # Create a figure with two subplots side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), \n",
    "                                      gridspec_kw={'width_ratios': [3, 1]})\n",
    "        \n",
    "        # Extract systems\n",
    "        all_systems = df_summary['System'].unique()\n",
    "        \n",
    "        # Separate 'h2' from other systems\n",
    "        other_systems = [s for s in all_systems if s != 'h2']\n",
    "        \n",
    "        # First subplot: all systems except 'h2'\n",
    "        x_positions_main = np.arange(len(other_systems))\n",
    "        lr_medians_main = []\n",
    "        lr_iqrs_main = []\n",
    "        dl_medians_main = []\n",
    "        dl_iqrs_main = []\n",
    "        \n",
    "        # Collect data for other systems\n",
    "        for system in other_systems:\n",
    "            system_data = df_summary[df_summary['System'] == system]\n",
    "            \n",
    "            # Aggregate across datasets if multiple exist per system\n",
    "            # For median, take the median of medians\n",
    "            lr_median = np.median(system_data[f'LR_{metric}_median'])\n",
    "            \n",
    "            # For IQR aggregation, we take the average IQR \n",
    "            # (could also use median of IQRs as an alternative)\n",
    "            lr_iqr = system_data[f'LR_{metric}_iqr'].mean()\n",
    "            \n",
    "            dl_median = np.median(system_data[f'DL_{metric}_median'])\n",
    "            dl_iqr = system_data[f'DL_{metric}_iqr'].mean()\n",
    "            \n",
    "            lr_medians_main.append(lr_median)\n",
    "            lr_iqrs_main.append(lr_iqr)\n",
    "            dl_medians_main.append(dl_median)\n",
    "            dl_iqrs_main.append(dl_iqr)\n",
    "        \n",
    "        # Width of the bars\n",
    "        bar_width = 0.35\n",
    "        \n",
    "        # Create bars with error bars for main systems\n",
    "        # For IQR, we display median +/- IQR/2\n",
    "        bars1 = ax1.bar(x_positions_main - bar_width/2, lr_medians_main, bar_width, \n",
    "               yerr=[iqr/2 for iqr in lr_iqrs_main], label='Linear Regression', alpha=0.7, capsize=5)\n",
    "        bars2 = ax1.bar(x_positions_main + bar_width/2, dl_medians_main, bar_width, \n",
    "               yerr=[iqr/2 for iqr in dl_iqrs_main], label='Deep Learning', alpha=0.7, capsize=5)\n",
    "        \n",
    "        # Add data labels to bars\n",
    "        for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "            height1 = bar1.get_height()\n",
    "            height2 = bar2.get_height()\n",
    "            ax1.text(bar1.get_x() + bar1.get_width()/2., height1 + lr_iqrs_main[i]/2 + 0.01,\n",
    "                    f'{height1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "            ax1.text(bar2.get_x() + bar2.get_width()/2., height2 + dl_iqrs_main[i]/2 + 0.01,\n",
    "                    f'{height2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "        # Set labels and title for main systems\n",
    "        ax1.set_xlabel('System')\n",
    "        ax1.set_ylabel(metric)\n",
    "        ax1.set_title(f'{metric} Comparison Across Systems (Median with IQR)')\n",
    "        ax1.set_xticks(x_positions_main)\n",
    "        ax1.set_xticklabels(other_systems, rotation=45)\n",
    "        ax1.legend(loc='best')\n",
    "        \n",
    "        # Second subplot: only 'h2'\n",
    "        if 'h2' in all_systems:\n",
    "            h2_data = df_summary[df_summary['System'] == 'h2']\n",
    "            \n",
    "            # Calculate metrics for h2\n",
    "            lr_median_h2 = np.median(h2_data[f'LR_{metric}_median'])\n",
    "            lr_iqr_h2 = h2_data[f'LR_{metric}_iqr'].mean()\n",
    "            dl_median_h2 = np.median(h2_data[f'DL_{metric}_median'])\n",
    "            dl_iqr_h2 = h2_data[f'DL_{metric}_iqr'].mean()\n",
    "            \n",
    "            # Create bars for h2\n",
    "            bar1_h2 = ax2.bar(0 - bar_width/2, lr_median_h2, bar_width, \n",
    "                   yerr=lr_iqr_h2/2, label='Linear Regression', alpha=0.7, capsize=5)\n",
    "            bar2_h2 = ax2.bar(0 + bar_width/2, dl_median_h2, bar_width, \n",
    "                   yerr=dl_iqr_h2/2, label='Deep Learning', alpha=0.7, capsize=5)\n",
    "            \n",
    "            # Add data labels\n",
    "            ax2.text(bar1_h2[0].get_x() + bar1_h2[0].get_width()/2., lr_median_h2 + lr_iqr_h2/2 + 0.01,\n",
    "                    f'{lr_median_h2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "            ax2.text(bar2_h2[0].get_x() + bar2_h2[0].get_width()/2., dl_median_h2 + dl_iqr_h2/2 + 0.01,\n",
    "                    f'{dl_median_h2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Set labels for h2\n",
    "            ax2.set_xlabel('System')\n",
    "            ax2.set_title('h2 System')\n",
    "            ax2.set_xticks([0])\n",
    "            ax2.set_xticklabels(['h2'])\n",
    "        \n",
    "        # Add note about which direction is better\n",
    "        if metric == 'R2':\n",
    "            plt.figtext(0.01, 0.01, \"Higher is better\", style='italic', fontsize=10)\n",
    "        else:\n",
    "            plt.figtext(0.01, 0.01, \"Lower is better\", style='italic', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/visualizations/{metric}_all_systems_median_iqr.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create improvement percentage visualization across all systems\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Prepare data for plotting improvement percentages based on medians\n",
    "    improvement_data = []\n",
    "    \n",
    "    for system in all_systems:\n",
    "        system_data = df_summary[df_summary['System'] == system]\n",
    "        for metric in metrics:\n",
    "            # Use median of improvement percentages for each system\n",
    "            improvement = system_data[f'{metric}_improvement'].median()\n",
    "            \n",
    "            improvement_data.append({\n",
    "                'System': system,\n",
    "                'Metric': metric,\n",
    "                'Improvement (%)': improvement\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    imp_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    # Create grouped bar plot for improvements\n",
    "    ax = sns.barplot(x='System', y='Improvement (%)', hue='Metric', data=imp_df)\n",
    "    \n",
    "    # Add data labels to bars\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width()/2., \n",
    "                height + (0.5 if height >= 0 else -2.5),\n",
    "                f'{height:.1f}%', \n",
    "                ha='center', fontsize=8)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.title('Improvement Percentages Across Systems and Metrics (Based on Median Values)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/visualizations/all_improvements_median.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = create_summary()\n",
    "create_metric_plots(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary():\n",
    "    summary_data = []\n",
    "    for (system, dataset), group in df_detailed.groupby(['System', 'Dataset']):\n",
    "        row = {'System': system, 'Dataset': dataset}\n",
    "        \n",
    "        # Calculate averages and std devs\n",
    "        for prefix in ['LR', 'DL']:\n",
    "            for metric in ['MAPE', 'MAE', 'RMSE', 'R2']:\n",
    "                col = f\"{prefix}_{metric}\"\n",
    "                if col in df_detailed.columns:\n",
    "                    row[f\"{col}_mean\"] = group[col].mean()\n",
    "                    row[f\"{col}_std\"] = group[col].std()\n",
    "        \n",
    "        # Calculate improvements\n",
    "        for metric in ['MAPE', 'MAE', 'RMSE']:\n",
    "            lr_mean = row[f\"LR_{metric}_mean\"]\n",
    "            dl_mean = row[f\"DL_{metric}_mean\"]\n",
    "            # For error metrics, lower is better\n",
    "            row[f\"{metric}_improvement\"] = ((lr_mean - dl_mean) / lr_mean) * 100\n",
    "        \n",
    "        # For R2, higher is better\n",
    "        if row['LR_R2_mean'] > 0:\n",
    "            row['R2_improvement'] = ((row['DL_R2_mean'] - row['LR_R2_mean']) / abs(row['LR_R2_mean'])) * 100\n",
    "        else:\n",
    "            row['R2_improvement'] = row['DL_R2_mean'] - row['LR_R2_mean']\n",
    "        \n",
    "        # Calculate statistical significance\n",
    "        for metric in ['MAPE', 'MAE', 'RMSE', 'R2']:\n",
    "            lr_col = f\"LR_{metric}\"\n",
    "            dl_col = f\"DL_{metric}\"\n",
    "            if lr_col in df_detailed.columns and dl_col in df_detailed.columns:\n",
    "                try:\n",
    "                    if metric == 'R2':\n",
    "                        # For R2, higher is better\n",
    "                        stat, p_value = stats.wilcoxon(group[dl_col], group[lr_col], alternative='greater')\n",
    "                    else:\n",
    "                        # For error metrics, lower is better\n",
    "                        stat, p_value = stats.wilcoxon(group[lr_col], group[dl_col], alternative='greater')\n",
    "                    \n",
    "                    row[f\"{metric}_p_value\"] = p_value\n",
    "                    row[f\"{metric}_significant\"] = p_value < 0.05\n",
    "                except:\n",
    "                    row[f\"{metric}_p_value\"] = np.nan\n",
    "                    row[f\"{metric}_significant\"] = False\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "df_summary = create_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created at 'results/tables/simple_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "def create_system_summary_table(df_summary):\n",
    "    \"\"\"\n",
    "    Create a summary table showing metrics and improvements for each system\n",
    "    \"\"\"\n",
    "    # Prepare the data for the summary table\n",
    "    table_data = []\n",
    "    \n",
    "    # Process each system\n",
    "    for system in df_summary['System'].unique():\n",
    "        system_data = df_summary[df_summary['System'] == system]\n",
    "        \n",
    "        # Calculate average metrics and std across datasets for this system\n",
    "        row = {'System': system}\n",
    "        \n",
    "        # Calculate metrics with standard deviations\n",
    "        for metric in ['MAPE', 'MAE', 'RMSE', 'R2']:\n",
    "            # Average values\n",
    "            lr_mean = system_data[f'LR_{metric}_mean'].mean()\n",
    "            dl_mean = system_data[f'DL_{metric}_mean'].mean()\n",
    "            \n",
    "            # Standard deviations\n",
    "            lr_std = system_data[f'LR_{metric}_std'].mean()\n",
    "            dl_std = system_data[f'DL_{metric}_std'].mean()\n",
    "            \n",
    "            # Format with 2 decimal places\n",
    "            row[f'LR_{metric}'] = f\"{lr_mean:.2f} ± {lr_std:.2f}\"\n",
    "            row[f'DL_{metric}'] = f\"{dl_mean:.2f} ± {dl_std:.2f}\"\n",
    "            \n",
    "            # Calculate improvement\n",
    "            imp = system_data[f'{metric}_improvement'].mean()\n",
    "            row[f'{metric}_Improvement'] = f\"{imp:.2f}%\"\n",
    "            \n",
    "            # Count significant improvements\n",
    "            sig_count = sum(system_data[f'{metric}_significant'])\n",
    "            total_count = len(system_data)\n",
    "            row[f'{metric}_Significant'] = f\"{sig_count}/{total_count}\"\n",
    "            \n",
    "            # Is DL better?\n",
    "            row[f'{metric}_Better'] = \"Yes\" if imp > 0 else \"No\"\n",
    "        \n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Add overall average row\n",
    "    overall_row = {'System': 'Overall'}\n",
    "    for metric in ['MAPE', 'MAE', 'RMSE', 'R2']:\n",
    "        # Average values across all systems\n",
    "        all_lr_means = [float(row[f'LR_{metric}'].split(' ')[0]) for row in table_data]\n",
    "        all_dl_means = [float(row[f'DL_{metric}'].split(' ')[0]) for row in table_data]\n",
    "        \n",
    "        overall_lr = np.mean(all_lr_means)\n",
    "        overall_dl = np.mean(all_dl_means)\n",
    "        \n",
    "        overall_row[f'LR_{metric}'] = f\"{overall_lr:.2f}\"\n",
    "        overall_row[f'DL_{metric}'] = f\"{overall_dl:.2f}\"\n",
    "        \n",
    "        # Overall improvement\n",
    "        all_imps = [float(row[f'{metric}_Improvement'].replace('%', '')) for row in table_data]\n",
    "        overall_imp = np.mean(all_imps)\n",
    "        overall_row[f'{metric}_Improvement'] = f\"{overall_imp:.2f}%\"\n",
    "        \n",
    "        # Count total significant improvements\n",
    "        sig_counts = [int(row[f'{metric}_Significant'].split('/')[0]) for row in table_data]\n",
    "        total_counts = [int(row[f'{metric}_Significant'].split('/')[1]) for row in table_data]\n",
    "        overall_row[f'{metric}_Significant'] = f\"{sum(sig_counts)}/{sum(total_counts)}\"\n",
    "        \n",
    "        # Overall better?\n",
    "        overall_row[f'{metric}_Better'] = \"Yes\" if overall_imp > 0 else \"No\"\n",
    "    \n",
    "    table_data.append(overall_row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_table = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_table.to_csv('results/tables/simple_summary.csv', index=False)\n",
    "    \n",
    "    print(\"Table created at 'results/tables/simple_summary.csv'\")\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "# Call this function after creating df_summary\n",
    "simple_summary = create_system_summary_table(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon Signed-Rank Test Results:\n",
      "=================================\n",
      "\n",
      "Metric: MAPE (better if lower)\n",
      "  DL mean: 0.2689\n",
      "  LR mean: 1.7680\n",
      "  Difference: -1.4991\n",
      "  Better on average: DL\n",
      "  Wilcoxon statistic: 423970.0\n",
      "  P-value: 0.0000\n",
      "  Statistically significant: Yes\n",
      "  Conclusion: DL is significantly better for MAPE\n",
      "\n",
      "Metric: MAE (better if lower)\n",
      "  DL mean: 93.1182\n",
      "  LR mean: 120.1139\n",
      "  Difference: -26.9957\n",
      "  Better on average: DL\n",
      "  Wilcoxon statistic: 410963.0\n",
      "  P-value: 0.0000\n",
      "  Statistically significant: Yes\n",
      "  Conclusion: DL is significantly better for MAE\n",
      "\n",
      "Metric: RMSE (better if lower)\n",
      "  DL mean: 117.6779\n",
      "  LR mean: 142.5734\n",
      "  Difference: -24.8956\n",
      "  Better on average: DL\n",
      "  Wilcoxon statistic: 344675.0\n",
      "  P-value: 0.0000\n",
      "  Statistically significant: Yes\n",
      "  Conclusion: DL is significantly better for RMSE\n",
      "\n",
      "Metric: R2 (better if higher)\n",
      "  DL mean: 0.7184\n",
      "  LR mean: 0.5865\n",
      "  Difference: 0.1319\n",
      "  Better on average: DL\n",
      "  Wilcoxon statistic: 360442.0\n",
      "  P-value: 0.0000\n",
      "  Statistically significant: Yes\n",
      "  Conclusion: DL is significantly better for R2\n",
      "\n",
      "Results saved to wilcoxon_test_results.csv\n",
      "\n",
      "\n",
      "System-Level Analysis:\n",
      "======================\n",
      "\n",
      "System: batlik\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.0000, significant)\n",
      "  R2: DL better (p=0.0000, significant)\n",
      "\n",
      "System: dconvert\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.0000, significant)\n",
      "  R2: DL better (p=0.0000, significant)\n",
      "\n",
      "System: h2\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.1900, not significant)\n",
      "  R2: DL better (p=0.2665, not significant)\n",
      "\n",
      "System: jump3r\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.0000, significant)\n",
      "  R2: DL better (p=0.0000, significant)\n",
      "\n",
      "System: kanzi\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.0000, significant)\n",
      "  R2: DL better (p=0.0000, significant)\n",
      "\n",
      "System: lrzip\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: LR better (p=1.0000, not significant)\n",
      "  R2: LR better (p=1.0000, not significant)\n",
      "\n",
      "System: x264\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.0000, significant)\n",
      "  R2: DL better (p=0.0000, significant)\n",
      "\n",
      "System: xz\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.0000, significant)\n",
      "  R2: DL better (p=0.0000, significant)\n",
      "\n",
      "System: z3\n",
      "  MAPE: DL better (p=0.0000, significant)\n",
      "  MAE: DL better (p=0.0000, significant)\n",
      "  RMSE: DL better (p=0.0000, significant)\n",
      "  R2: DL better (p=0.0000, significant)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('results/data/all_results_detailed.csv')\n",
    "\n",
    "# Define metrics and whether higher or lower values are better\n",
    "metrics = {\n",
    "    'MAPE': 'lower',  # Lower MAPE is better\n",
    "    'MAE': 'lower',   # Lower MAE is better\n",
    "    'RMSE': 'lower',  # Lower RMSE is better\n",
    "    'R2': 'higher'    # Higher R2 is better\n",
    "}\n",
    "\n",
    "# Store results for table\n",
    "results = []\n",
    "\n",
    "print(\"Wilcoxon Signed-Rank Test Results:\")\n",
    "print(\"=================================\")\n",
    "\n",
    "for metric, better in metrics.items():\n",
    "    dl_values = df[f'DL_{metric}']\n",
    "    lr_values = df[f'LR_{metric}']\n",
    "    \n",
    "    # Determine which model performs better on average\n",
    "    if better == 'lower':\n",
    "        avg_better = 'DL' if np.mean(dl_values) < np.mean(lr_values) else 'LR'\n",
    "    else:  # higher is better\n",
    "        avg_better = 'DL' if np.mean(dl_values) > np.mean(lr_values) else 'LR'\n",
    "    \n",
    "    # Perform the Wilcoxon test with appropriate alternative hypothesis\n",
    "    if better == 'lower':\n",
    "        # For metrics where lower is better\n",
    "        statistic, p_value = wilcoxon(lr_values, dl_values, alternative='greater')\n",
    "    else:  # better == 'higher'\n",
    "        # For metrics where higher is better\n",
    "        statistic, p_value = wilcoxon(dl_values, lr_values, alternative='greater')\n",
    "    \n",
    "    # Determine if the difference is statistically significant\n",
    "    is_significant = p_value < 0.05\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Metric': metric,\n",
    "        'DL Mean': np.mean(dl_values),\n",
    "        'LR Mean': np.mean(lr_values),\n",
    "        'Difference': np.mean(dl_values - lr_values),\n",
    "        'Better Model': avg_better,\n",
    "        'P-value': p_value,\n",
    "        'Significant': is_significant\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMetric: {metric} (better if {better})\")\n",
    "    print(f\"  DL mean: {np.mean(dl_values):.4f}\")\n",
    "    print(f\"  LR mean: {np.mean(lr_values):.4f}\")\n",
    "    print(f\"  Difference: {np.mean(dl_values - lr_values):.4f}\")\n",
    "    print(f\"  Better on average: {avg_better}\")\n",
    "    print(f\"  Wilcoxon statistic: {statistic}\")\n",
    "    print(f\"  P-value: {p_value:.4f}\")\n",
    "    print(f\"  Statistically significant: {'Yes' if is_significant else 'No'}\")\n",
    "    \n",
    "    if is_significant:\n",
    "        conclusion = f\"{avg_better} is significantly better for {metric}\"\n",
    "    else:\n",
    "        conclusion = f\"No significant difference between DL and LR for {metric}\"\n",
    "    print(f\"  Conclusion: {conclusion}\")\n",
    "    \n",
    "\n",
    "# Create a results table\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('results/tables/wilcoxon_test_results.csv', index=False)\n",
    "print(\"\\nResults saved to wilcoxon_test_results.csv\")\n",
    "\n",
    "# System-level analysis\n",
    "print(\"\\n\\nSystem-Level Analysis:\")\n",
    "print(\"======================\")\n",
    "\n",
    "systems = df['System'].unique()\n",
    "for system in systems:\n",
    "    system_df = df[df['System'] == system]\n",
    "    \n",
    "    print(f\"\\nSystem: {system}\")\n",
    "    for metric, better in metrics.items():\n",
    "        system_dl = system_df[f'DL_{metric}']\n",
    "        system_lr = system_df[f'LR_{metric}']\n",
    "        \n",
    "        # Perform the test\n",
    "        try:\n",
    "            if better == 'lower':\n",
    "                statistic, p_value = wilcoxon(system_lr, system_dl, alternative='greater')\n",
    "            else:\n",
    "                statistic, p_value = wilcoxon(system_dl, system_lr, alternative='greater')\n",
    "                \n",
    "            is_significant = p_value < 0.05\n",
    "            \n",
    "            if better == 'lower':\n",
    "                avg_better = 'DL' if np.mean(system_dl) < np.mean(system_lr) else 'LR'\n",
    "            else:\n",
    "                avg_better = 'DL' if np.mean(system_dl) > np.mean(system_lr) else 'LR'\n",
    "                \n",
    "            print(f\"  {metric}: {avg_better} better (p={p_value:.4f}, {'significant' if is_significant else 'not significant'})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {metric}: Error in test - {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
